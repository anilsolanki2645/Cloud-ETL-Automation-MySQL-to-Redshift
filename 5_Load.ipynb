{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLNdjo-ze1jr"
      },
      "source": [
        "# Load Data Step:\n",
        "* AWS MySQL Backup, S3 Upload, Redshift Cluster Setup, Table Creation, upload data using S3 cp command and Cleanup\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S7zSSj3HdqyZ"
      },
      "source": [
        "* Mounts Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nxTsgcC44DRQ",
        "outputId": "be5ef4ab-04a9-4163-c377-220243953fc5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cqIYL9Ld0mh"
      },
      "source": [
        "* Installing Required Python Packages: boto3 and paramiko\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lJXw85F74KPk",
        "outputId": "56a61609-4e75-43ee-e7cf-63b661d0b005"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.34.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting paramiko\n",
            "  Downloading paramiko-3.3.1-py3-none-any.whl (224 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.8/224.8 kB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore<1.35.0,>=1.34.1 (from boto3)\n",
            "  Downloading botocore-1.34.1-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m68.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.10.0,>=0.9.0 (from boto3)\n",
            "  Downloading s3transfer-0.9.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bcrypt>=3.2 (from paramiko)\n",
            "  Downloading bcrypt-4.1.2-cp39-abi3-manylinux_2_28_x86_64.whl (698 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m698.9/698.9 kB\u001b[0m \u001b[31m46.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cryptography>=3.3 in /usr/local/lib/python3.10/dist-packages (from paramiko) (41.0.7)\n",
            "Collecting pynacl>=1.5 (from paramiko)\n",
            "  Downloading PyNaCl-1.5.0-cp36-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.manylinux_2_24_x86_64.whl (856 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m856.7/856.7 kB\u001b[0m \u001b[31m50.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.1->boto3) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore<1.35.0,>=1.34.1->boto3) (2.0.7)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=3.3->paramiko) (1.16.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=3.3->paramiko) (2.21)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore<1.35.0,>=1.34.1->boto3) (1.16.0)\n",
            "Installing collected packages: jmespath, bcrypt, pynacl, botocore, s3transfer, paramiko, boto3\n",
            "Successfully installed bcrypt-4.1.2 boto3-1.34.1 botocore-1.34.1 jmespath-1.0.1 paramiko-3.3.1 pynacl-1.5.0 s3transfer-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 paramiko"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vtjfbFbBd7l0"
      },
      "source": [
        "* AWS and Paramiko Library Importation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "5b7J2bo78ros"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import paramiko\n",
        "import psycopg2\n",
        "import time\n",
        "import subprocess"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxmnJAnkgMO3"
      },
      "source": [
        "* AWS Credential and Parameter Configuration for Redshift and S3\n",
        "* you can change parameter value according to your need"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "rBunQLWIz6dB"
      },
      "outputs": [],
      "source": [
        "#AWS Credential with Region\n",
        "aws_access_key = 'your aws_access_key'\n",
        "aws_secret_key = 'your aws_secret_key'\n",
        "aws_region = 'ap-south-1'\n",
        "\n",
        "# IAM role ARN\n",
        "# go to IAM Role and search this iam_role_arn role value and then you will find actual role copy arn of that role and past here\n",
        "iam_role_arn = 'AmazonRedshift-CommandsAccessRole-20231213T202016' # must be replace this role\n",
        "\n",
        "# S3 bucket parameter configuration:\n",
        "s3_bucket_name = 'etlpro'\n",
        "s3_file_path = 'Backup_imdb.csv'\n",
        "\n",
        "# Specify Redshift parameters\n",
        "cluster_identifier = 'clustername'\n",
        "database_name = 'imdb'\n",
        "master_username = 'admin'\n",
        "master_password = 'l4Fwt@!~`=gg<Uff&^%$hyru' # Note : password must be strong either it will be show an error \n",
        "node_type = 'dc2.large'\n",
        "number_of_nodes = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HJfvdgEfn52"
      },
      "source": [
        "* SSH into AWS EC2 Instance and Execute Commands for MySQL Backup and S3 Upload\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRG_4qUMsITy"
      },
      "source": [
        "* Note : if File uploading error then attach S3 permission role to ec2, still not resolve than do aws configure in ec2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKsYq8RF4H1A",
        "outputId": "38d9c90e-022a-4829-bfaf-38b8a76d8e26"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Command Output for 'sudo mysqldump -u etl -pAdmin@123 imdb > /tmp/Backup_imdb.sql':\n",
            "\n",
            "Command Error for 'sudo mysqldump -u etl -pAdmin@123 imdb > /tmp/Backup_imdb.sql':\n",
            "mysqldump: [Warning] Using a password on the command line interface can be insecure.\n",
            "mysqldump: Error: 'Access denied; you need (at least one of) the PROCESS privilege(s) for this operation' when trying to dump tablespaces\n",
            "\n",
            "Command Output for 'sudo apt-get update':\n",
            "Hit:1 http://ap-south-1.ec2.archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:2 http://ap-south-1.ec2.archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:3 http://ap-south-1.ec2.archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:4 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Reading package lists...\n",
            "\n",
            "Command Error for 'sudo apt-get update':\n",
            "\n",
            "Command Output for 'sudo apt-get upgrade -y':\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "Calculating upgrade...\n",
            "The following packages have been kept back:\n",
            "  linux-aws linux-headers-aws linux-image-aws python3-update-manager\n",
            "  systemd-hwe-hwdb ubuntu-advantage-tools update-manager-core\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "\n",
            "Command Error for 'sudo apt-get upgrade -y':\n",
            "\n",
            "Command Output for 'sudo apt-get install awscli -y':\n",
            "Reading package lists...\n",
            "Building dependency tree...\n",
            "Reading state information...\n",
            "awscli is already the newest version (1.22.34-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 7 not upgraded.\n",
            "\n",
            "Command Error for 'sudo apt-get install awscli -y':\n",
            "\n",
            "Command Output for 'aws s3 cp /tmp/Backup_imdb.sql s3://etlpro/':\n",
            "upload: ../../tmp/Backup_imdb.sql to s3://etlpro/Backup_imdb.sql \n",
            "\n",
            "Command Error for 'aws s3 cp /tmp/Backup_imdb.sql s3://etlpro/':\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# SSH into the instance (you might need to replace the key file path and username)\n",
        "ssh_client = paramiko.SSHClient()\n",
        "ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
        "\n",
        "# if you same .pem file in other location then replace the path\n",
        "key_file_path = '/content/drive/MyDrive/Colab Notebooks/ETL_KP.pem'\n",
        "public_ip_address = 'Your_Ec2_instance_public_ip_address'\n",
        "\n",
        "# while you use different type of operating system then must be change the username\n",
        "ssh_client.connect(public_ip_address, username='ubuntu', key_filename=key_file_path)\n",
        "\n",
        "# commands for Backup a database, Download .sql file, and upload to S3\n",
        "# must be change sql password in commands at your_sql_user_password\n",
        "commands = [\n",
        "    'sudo mysqldump -u etl -pyour_sql_user_password imdb > /tmp/Backup_imdb.sql',\n",
        "    'sudo apt-get update',\n",
        "    'sudo apt-get upgrade -y',\n",
        "    'sudo apt-get install awscli -y',\n",
        "    'aws s3 cp /tmp/Backup_imdb.sql s3://etlpro/'\n",
        "]\n",
        "\n",
        "# Execute each command\n",
        "for command in commands:\n",
        "    stdin, stdout, stderr = ssh_client.exec_command(command)\n",
        "    output = stdout.read().decode('utf-8')\n",
        "    error = stderr.read().decode('utf-8')\n",
        "\n",
        "    # Print the output and error messages\n",
        "    print(f\"Command Output for '{command}':\\n{output}\")\n",
        "    print(f\"Command Error for '{command}':\\n{error}\")\n",
        "\n",
        "# Close the SSH connection\n",
        "ssh_client.close()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vHBEBOC1gZUG"
      },
      "source": [
        "* AWS Redshift Client Initialization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "i2IdJiwF0kJm"
      },
      "outputs": [],
      "source": [
        "# Create object for Redshift cluster\n",
        "redshift_client = boto3.client('redshift', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key, region_name=aws_region)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t7i6PCvGglWJ"
      },
      "source": [
        "* Creating AWS Redshift Cluster:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gshb0zZldN_m",
        "outputId": "fc49a011-f05e-452e-ac9c-b59b7477ee49"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster etlproject is being created. Please wait...\n",
            "Cluster etlproject has been created successfully.\n"
          ]
        }
      ],
      "source": [
        "response = redshift_client.create_cluster(\n",
        "    ClusterIdentifier=cluster_identifier,\n",
        "    NodeType=node_type,\n",
        "    MasterUsername=master_username,\n",
        "    MasterUserPassword=master_password,\n",
        "    NumberOfNodes=number_of_nodes,\n",
        "    DBName=database_name,\n",
        "    IamRoles=[iam_role_arn,]\n",
        ")\n",
        "\n",
        "print(f\"Cluster {cluster_identifier} is being created. Please wait...\")\n",
        "\n",
        "# Wait for the cluster to be available\n",
        "redshift_client.get_waiter('cluster_available').wait(ClusterIdentifier=cluster_identifier)\n",
        "\n",
        "print(f\"Cluster {cluster_identifier} has been created successfully.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76_h0307g0iE"
      },
      "source": [
        "* Describing AWS Redshift Cluster:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l4bh05HClepD",
        "outputId": "a37b5ba7-867b-4108-9d1b-9647f095022d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "etlproject.ciajlcnv4lg2.ap-south-1.redshift.amazonaws.com\n",
            "5439\n",
            "available\n",
            "[{'IamRoleArn': 'arn:aws:iam::475109786741:role/service-role/AmazonRedshift-CommandsAccessRole-20231213T202016', 'ApplyStatus': 'in-sync'}]\n",
            "{'ClusterIdentifier': 'etlproject', 'NodeType': 'dc2.large', 'ClusterStatus': 'available', 'ClusterAvailabilityStatus': 'Unavailable', 'MasterUsername': 'admin', 'DBName': 'imdb', 'Endpoint': {'Address': 'etlproject.ciajlcnv4lg2.ap-south-1.redshift.amazonaws.com', 'Port': 5439}, 'ClusterCreateTime': datetime.datetime(2023, 12, 15, 19, 7, 37, 116000, tzinfo=tzlocal()), 'AutomatedSnapshotRetentionPeriod': 1, 'ManualSnapshotRetentionPeriod': -1, 'ClusterSecurityGroups': [], 'VpcSecurityGroups': [{'VpcSecurityGroupId': 'sg-0dae590430548e22c', 'Status': 'active'}], 'ClusterParameterGroups': [{'ParameterGroupName': 'default.redshift-1.0', 'ParameterApplyStatus': 'in-sync'}], 'ClusterSubnetGroupName': 'default', 'VpcId': 'vpc-051c6bf2ff976c0e8', 'AvailabilityZone': 'ap-south-1c', 'PreferredMaintenanceWindow': 'sun:06:00-sun:06:30', 'PendingModifiedValues': {}, 'ClusterVersion': '1.0', 'AllowVersionUpgrade': True, 'NumberOfNodes': 2, 'PubliclyAccessible': True, 'Encrypted': False, 'ClusterPublicKey': 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABAQC4haGyvSlygUhO1fIkacXbLWrcQqD0pvd3PfY8VZyPCohIBdkvM8bXDINvvHzGnSiTVVS885VGyWHnVNyfnmwxuCq1yBB4aSlZvyI3qU32Bv+thVFz8sJa9h5771YxtxixM4TwOmp31rSE1zQZ++8lc/AHzTitZsPmliT09GGWPM0tPWyVXc+Ct89ZLQx/b3FCxpl8rzCnqlgi90ShZxXZwF5LSMDWKVuK3rDgitzJGQomw4/vHBpepaHfirgPpCN8dYBuysGDpHnE+OfCE/S8h3jSjhdfxOe27feLpQS3mUR35RSlr/2xcz06hhc7yx+OaXjTlgbSCElJkv1lfdYX Amazon-Redshift\\n', 'ClusterNodes': [{'NodeRole': 'LEADER', 'PrivateIPAddress': '172.31.31.115', 'PublicIPAddress': '13.200.185.234'}, {'NodeRole': 'COMPUTE-0', 'PrivateIPAddress': '172.31.30.50', 'PublicIPAddress': '43.204.125.21'}, {'NodeRole': 'COMPUTE-1', 'PrivateIPAddress': '172.31.29.243', 'PublicIPAddress': '13.200.171.78'}], 'ClusterRevisionNumber': '60854', 'Tags': [], 'EnhancedVpcRouting': False, 'IamRoles': [{'IamRoleArn': 'arn:aws:iam::475109786741:role/service-role/AmazonRedshift-CommandsAccessRole-20231213T202016', 'ApplyStatus': 'in-sync'}], 'MaintenanceTrackName': 'current', 'ElasticResizeNumberOfNodeOptions': '[4]', 'DeferredMaintenanceWindows': [], 'NextMaintenanceWindowStartTime': datetime.datetime(2023, 12, 17, 6, 0, tzinfo=tzlocal()), 'AvailabilityZoneRelocationStatus': 'disabled', 'ClusterNamespaceArn': 'arn:aws:redshift:ap-south-1:475109786741:namespace:b1223c39-d4fd-4c8d-a2fd-3802f3796e60', 'AquaConfiguration': {'AquaStatus': 'disabled', 'AquaConfigurationStatus': 'auto'}, 'MultiAZ': 'Disabled'}\n"
          ]
        }
      ],
      "source": [
        "# Describe the cluster to obtain its details\n",
        "describe_cluster = redshift_client.describe_clusters(ClusterIdentifier=cluster_identifier)\n",
        "\n",
        "# Connect to the cluster and run example queries\n",
        "cluster_endpoint = describe_cluster['Clusters'][0]['Endpoint']['Address']\n",
        "port = describe_cluster['Clusters'][0]['Endpoint']['Port']\n",
        "cluster_status = describe_cluster['Clusters'][0]['ClusterStatus']\n",
        "cluster = describe_cluster['Clusters'][0]\n",
        "cluster_role = describe_cluster['Clusters'][0]['IamRoles']\n",
        "\n",
        "print(cluster_endpoint)\n",
        "print(port)\n",
        "print(cluster_status)\n",
        "print(cluster_role)\n",
        "print(cluster)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qSnSCr95iKRI"
      },
      "source": [
        "* Creating Connection to AWS Redshift Cluster\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U9DW0ipp1SdJ",
        "outputId": "9625c243-9407-4713-9f06-1f9a4afa7e19"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Connection established successfully!\n"
          ]
        }
      ],
      "source": [
        "# Create a connection string\n",
        "conn_string = f\"dbname={database_name} user={master_username} password={master_password} host={cluster_endpoint} port={port}\"\n",
        "\n",
        "conn = psycopg2.connect(conn_string)\n",
        "\n",
        "print(\"Connection established successfully!\")\n",
        "\n",
        "cursor = conn.cursor()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_rMFduFs807"
      },
      "source": [
        "* Redshift Table Creation and Data Loading\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qiTcljn5955c"
      },
      "outputs": [],
      "source": [
        "# CREATE TABLE command to the Redshift cluster\n",
        "# if your csv data are different then please change the table schema\n",
        "create_table_command = \"\"\"\n",
        "CREATE TABLE IF NOT EXISTS imdb.public.movie_data (\n",
        "    movie_name VARCHAR(255) NOT NULL UNIQUE,\n",
        "    year_of_release INTEGER NOT NULL,\n",
        "    watch_time VARCHAR(30) NOT NULL,\n",
        "    rating REAL NOT NULL,\n",
        "    metascore INTEGER NOT NULL,\n",
        "    votes INTEGER NOT NULL,\n",
        "    avg_rating REAL NOT NULL,\n",
        "    id INTEGER PRIMARY KEY NOT NULL\n",
        ");\n",
        "\"\"\"\n",
        "\n",
        "# Execute the CREATE TABLE command\n",
        "cursor.execute(create_table_command)\n",
        "\n",
        "# Generate the COPY command\n",
        "copy_command = f\"\"\"\n",
        "COPY imdb.public.movie_data\n",
        "FROM 's3://{s3_bucket_name}/{s3_file_path}'\n",
        "IAM_ROLE '{iam_role_arn}'\n",
        "FORMAT AS CSV\n",
        "DELIMITER ','\n",
        "QUOTE '\"'\n",
        "IGNOREHEADER 1\n",
        "REGION AS 'ap-south-1';\n",
        "\"\"\"\n",
        "\n",
        "# Execute the COPY command to load data and automatically create the table\n",
        "cursor.execute(copy_command)\n",
        "\n",
        "# Commit the changes\n",
        "conn.commit()\n",
        "\n",
        "# Close the cursor and connection\n",
        "cursor.close()\n",
        "conn.close()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qooMm-kStmnZ"
      },
      "source": [
        "* Deleting AWS Redshift Cluster:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IqEupWudtTrn",
        "outputId": "c792f92e-962d-4ee3-a0a5-6a681289da03"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster etlproject is being deleted. Please wait...\n",
            "Cluster etlproject has been deleted successfully.\n"
          ]
        }
      ],
      "source": [
        "cluster_identifier_to_delete = cluster_identifier\n",
        "\n",
        "# Delete the Redshift cluster\n",
        "response = redshift_client.delete_cluster(ClusterIdentifier=cluster_identifier_to_delete, SkipFinalClusterSnapshot=True)\n",
        "print(f\"Cluster {cluster_identifier_to_delete} is being deleted. Please wait...\")\n",
        "\n",
        "# Wait for the cluster to be deleted\n",
        "redshift_client.get_waiter('cluster_deleted').wait(ClusterIdentifier=cluster_identifier_to_delete)\n",
        "\n",
        "print(f\"Cluster {cluster_identifier_to_delete} has been deleted successfully.\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
