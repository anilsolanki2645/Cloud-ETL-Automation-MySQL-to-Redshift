{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3XaUDZwoAgOa"
      },
      "source": [
        "# S3 Bucket Configuration : Create, Upload, Delete"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AnXclNANAQHG"
      },
      "source": [
        "* Mounts Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ub61055Gkje_",
        "outputId": "139944b5-738c-40cd-8355-31a3fc1716d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WOAvhdatdQng"
      },
      "source": [
        "* install and import boto3 and botocore library for connect with AWS"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sWWxcsyy-LiP",
        "outputId": "fc0ada21-fd6f-412f-a24c-d167d26054d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting boto3\n",
            "  Downloading boto3-1.34.1-py3-none-any.whl (139 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m139.3/139.3 kB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting botocore\n",
            "  Downloading botocore-1.34.1-py3-none-any.whl (11.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.8/11.8 MB\u001b[0m \u001b[31m45.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jmespath<2.0.0,>=0.7.1 (from boto3)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Collecting s3transfer<0.10.0,>=0.9.0 (from boto3)\n",
            "  Downloading s3transfer-0.9.0-py3-none-any.whl (82 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m82.0/82.0 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil<3.0.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from botocore) (2.8.2)\n",
            "Requirement already satisfied: urllib3<2.1,>=1.25.4 in /usr/local/lib/python3.10/dist-packages (from botocore) (2.0.7)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil<3.0.0,>=2.1->botocore) (1.16.0)\n",
            "Installing collected packages: jmespath, botocore, s3transfer, boto3\n",
            "Successfully installed boto3-1.34.1 botocore-1.34.1 jmespath-1.0.1 s3transfer-0.9.0\n"
          ]
        }
      ],
      "source": [
        "!pip install boto3 botocore"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "sbwFoL6rnlyS"
      },
      "outputs": [],
      "source": [
        "import boto3\n",
        "import time\n",
        "from botocore.exceptions import NoCredentialsError, PartialCredentialsError, EndpointConnectionError, ClientError"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tT5ObkujdozC"
      },
      "source": [
        "\n",
        "* Declare important variable\n",
        "* must be change the variable values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SPRZ2G4olUzA"
      },
      "outputs": [],
      "source": [
        "# AWS Credentials\n",
        "aws_access_key = 'your aws_access_key'\n",
        "aws_secret_key = 'your aws_secret_key'\n",
        "\n",
        "# S3 Bucket Name\n",
        "bucket_name = 'your bucket name'\n",
        "\n",
        "# AWS Region\n",
        "region_name = 'ap-south-1'  # Replace with your desired AWS region, e.g., 'us-east-1'\n",
        "\n",
        "# S3 Bucket and Object Configuration\n",
        "local_file_path = '/content/drive/MyDrive/Colab Notebooks/IMDB_Movie_Ratings.xlsx'  # Replace with the path to your local file\n",
        "s3_object_key = 'IMDB_Movie_Ratings.xlsx'  # Replace with the desired S3 object key"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lhk8g-mXC-8f"
      },
      "source": [
        "* AWS S3 Client Initialization with Access Keys and Region Configuration\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "warFled7ne1u"
      },
      "outputs": [],
      "source": [
        "# Create S3 client object with the specified region\n",
        "s3 = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key, region_name=region_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XqvoRaaBCFRl"
      },
      "source": [
        "* Function to Create an AWS S3 Bucket with Error Handling and Retries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "m3gQlH4DG38a"
      },
      "outputs": [],
      "source": [
        "# Function to Create an AWS S3 Bucket\n",
        "def create_s3_bucket(bucket_name, aws_access_key, aws_secret_key, region_name):\n",
        "    max_retries = 3  # Adjust the number of retries as needed\n",
        "    retry_count = 0\n",
        "\n",
        "    while retry_count < max_retries:\n",
        "        try:\n",
        "            # Check if the bucket already exists\n",
        "            existing_buckets = [bucket['Name'] for bucket in s3.list_buckets()['Buckets']]\n",
        "            if bucket_name not in existing_buckets:\n",
        "                # Create S3 bucket with the correct location constraint\n",
        "                s3.create_bucket(Bucket=bucket_name,\n",
        "                                CreateBucketConfiguration={'LocationConstraint': region_name})\n",
        "\n",
        "                print(f\"S3 bucket '{bucket_name}' created successfully in region '{region_name}'.\")\n",
        "                break\n",
        "\n",
        "            else:\n",
        "                print(f\"S3 bucket '{bucket_name}' already exists. Choose a different name or proceed with your logic.\")\n",
        "                break\n",
        "\n",
        "        except (NoCredentialsError, PartialCredentialsError) as e:\n",
        "            print(f\"Credentials not available or incorrect. {e}\")\n",
        "            break\n",
        "\n",
        "        except EndpointConnectionError as e:\n",
        "            print(f\"Error connecting to AWS endpoint. {e}\")\n",
        "            break\n",
        "\n",
        "        except ClientError as e:\n",
        "            if e.response['Error']['Code'] == 'OperationAborted':\n",
        "                retry_count += 1\n",
        "                print(f\"Retrying ({retry_count}/{max_retries})...\")\n",
        "                time.sleep(5)  # Introduce a delay between retries\n",
        "                continue\n",
        "\n",
        "            else:\n",
        "                print(f\"Error creating S3 bucket. {e}\")\n",
        "                break\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"An unexpected error occurred. {e}\")\n",
        "            retry_count += 1\n",
        "            print(f\"Retrying ({retry_count}/{max_retries})...\")\n",
        "            time.sleep(5)  # Introduce a delay between retries\n",
        "            continue\n",
        "\n",
        "    if retry_count == max_retries:\n",
        "        print(f\"Failed to create S3 bucket '{bucket_name}' after {max_retries} attempts.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoA9mnt9DQD4"
      },
      "source": [
        "* Run the create_s3_bucket() Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uq4dIsq8lOhZ",
        "outputId": "037a5d02-e95e-4925-f9fa-c368c8748a84"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "S3 bucket 'etlpro' created successfully in region 'ap-south-1'.\n"
          ]
        }
      ],
      "source": [
        "create_s3_bucket(bucket_name, aws_access_key, aws_secret_key, region_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qPY7pgQg9Dp"
      },
      "source": [
        "\n",
        "* Function to Upload a Local File to an AWS S3 Bucket"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "aZvYJ3cleBn9"
      },
      "outputs": [],
      "source": [
        "def upload_to_s3(local_file, bucket, s3_key):\n",
        "    try:\n",
        "        # Upload the file\n",
        "        s3.upload_file(local_file, bucket, s3_key)\n",
        "        print(f\"File '{local_file}' uploaded to '{bucket}' with key '{s3_key}' successfully.\")\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"The file '{local_file}' was not found.\")\n",
        "    except NoCredentialsError:\n",
        "        print(\"Credentials not available or incorrect.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QbsQbPn9D1MH"
      },
      "source": [
        "* Run the upload_to_s3() Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oO2qfmr0n5H9",
        "outputId": "2c3c99f1-704c-410a-835d-8f70bb9e3208"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "File '/content/drive/MyDrive/Colab Notebooks/IMDB_Movie_Ratings.xlsx' uploaded to 'etlpro' with key 'IMDB_Movie_Ratings.xlsx' successfully.\n"
          ]
        }
      ],
      "source": [
        "# Upload the file to S3\n",
        "upload_to_s3(local_file_path, bucket_name, s3_object_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8PZkGDNPp_Hf"
      },
      "source": [
        "* Function to Delete an AWS S3 Bucket with its Objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "32aqhmdepbi9"
      },
      "outputs": [],
      "source": [
        "def delete_s3_bucket(bucket_name, aws_access_key, aws_secret_key, aws_region):\n",
        "    try:\n",
        "        # List objects in the bucket\n",
        "        response = s3.list_objects_v2(Bucket=bucket_name)\n",
        "\n",
        "        # Check if the bucket is not empty\n",
        "        if 'Contents' in response:\n",
        "            # Delete all objects in the bucket\n",
        "            objects = [{'Key': obj['Key']} for obj in response['Contents']]\n",
        "            s3.delete_objects(Bucket=bucket_name, Delete={'Objects': objects})\n",
        "            print(f\"All objects deleted from bucket '{bucket_name}'.\")\n",
        "\n",
        "        # Delete the bucket\n",
        "        s3.delete_bucket(Bucket=bucket_name)\n",
        "        print(f\"Bucket '{bucket_name}' deleted successfully.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error deleting bucket: {e}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLeI2hkzEp4y"
      },
      "source": [
        "* Run the delete_s3_bucket() Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ohuhGHkphFx",
        "outputId": "f4c1a2ba-a1a0-4dd6-f3e8-631805fd3ee5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "All objects deleted from bucket 'etlpro'.\n",
            "Bucket 'etlpro' deleted successfully.\n"
          ]
        }
      ],
      "source": [
        "delete_s3_bucket(bucket_name, aws_access_key, aws_secret_key, region_name)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
